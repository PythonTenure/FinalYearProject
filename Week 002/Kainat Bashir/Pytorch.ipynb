{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "\n",
        "PyTorch is a machine learning library that simplifies the creation and training of neural networks, widely used for its flexibility and ease of use. It supports dynamic computation graphs, enabling intuitive model design and experimentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "MAcpE-kjwVML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tensors\n",
        "\n",
        "Everything in PyTorch is based on Tensor operations. A Tensor is a multi-dimensional matrix containing elements of a single data type:\n"
      ],
      "metadata": {
        "id": "mUT_7ZCPwY9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mWV8uTgo4Bl",
        "outputId": "0dd9e089-3626-4c3f-88f4-6a04ad6f4e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "empty(1): tensor([1.3452e-43])\n",
            "empty(3): tensor([ 1.4418e-15,  3.2863e-41, -8.0485e-07])\n",
            "empty(2,3): tensor([[ 1.4423e-15,  3.2863e-41, -8.0485e-07],\n",
            "        [ 4.4679e-41,  8.9683e-44,  0.0000e+00]])\n",
            "empty(2, 2, 3): tensor([[[7.0065e-45, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 1.4013e-45, 0.0000e+00]]])\n",
            "rand(5,3): tensor([[0.3082, 0.5944, 0.8576],\n",
            "        [0.5026, 0.5358, 0.8352],\n",
            "        [0.5365, 0.0569, 0.4760],\n",
            "        [0.3217, 0.8649, 0.5425],\n",
            "        [0.7246, 0.9956, 0.6134]])\n",
            "zeros(5,3): tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# torch.empty(size): uninitiallized\n",
        "x = torch.empty(1) # scalar\n",
        "print(\"empty(1):\", x)\n",
        "x = torch.empty(3) # vector\n",
        "print(\"empty(3):\",x)\n",
        "x = torch.empty(2, 3) # matrix\n",
        "print(\"empty(2,3):\",x)\n",
        "x = torch.empty(2, 2, 3) # tensor, 3 dimensions\n",
        "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
        "print(\"empty(2, 2, 3):\",x)\n",
        "\n",
        "# torch.rand(size): random numbers [0, 1]\n",
        "x = torch.rand(5, 3)\n",
        "print(\"rand(5,3):\", x)\n",
        "\n",
        "# torch.zeros(size), fill with 0\n",
        "# torch.ones(size), fill with 1\n",
        "x = torch.zeros(5, 3)\n",
        "print(\"zeros(5,3):\", x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check size\n",
        "print(\"size\", x.size())  # x.size(0)\n",
        "print(\"shape\", x.shape)  # x.shape[0]"
      ],
      "metadata": {
        "id": "Asq_Sgh7cJnN",
        "outputId": "e4b3f7c1-d15d-4885-94ba-178211f1bce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size torch.Size([5, 3])\n",
            "shape torch.Size([5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# check data type\n",
        "print(x.dtype)\n",
        "\n",
        "# specify types, float32 default\n",
        "x = torch.zeros(5, 3, dtype=torch.float16)\n",
        "print(x)\n",
        "\n",
        "# check type\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "DmDNPMrp_zLj",
        "outputId": "a06bcd71-9435-46c5-8e9d-d744fde00921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], dtype=torch.float16)\n",
            "torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A tensor is a fundamental data structure in PyTorch and other deep learning frameworks.\n",
        "# It is a multi-dimensional array, similar to NumPy arrays, but with additional capabilities specifically designed for deep learning.\n",
        "\n",
        "# construct from data\n",
        "x = torch.tensor([5.5, 3])\n",
        "print(x, x.dtype)"
      ],
      "metadata": {
        "id": "sj2VAgMM_2Vu",
        "outputId": "7bf110aa-0bfb-4c74-9091-5ae45ae55c8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.5000, 3.0000]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad argument\n",
        "# This will tell pytorch that it will need to calculate the gradients for this tensor\n",
        "# later in your optimization steps\n",
        "# i.e. this is a variable in your model that you want to optimize\n",
        "x = torch.tensor([5.5, 3], requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "uT-3O8wtcMeZ",
        "outputId": "1a79e07c-a214-438f-f4bd-2947e0fd7996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.5000, 3.0000], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Operations with Tensors"
      ],
      "metadata": {
        "id": "_cDRipePEhZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Operations\n",
        "x = torch.ones(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "\n",
        "# elementwise addition\n",
        "z = x + y\n",
        "# torch.add(x,y)\n",
        "\n",
        "# in place addition, everythin with a trailing underscore is an inplace operation\n",
        "# i.e. it will modify the variable\n",
        "\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "id": "3pjK4YL6ES9B",
        "outputId": "ea94324b-c016-40e2-fb83-188de7d695aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.5600, 0.5209],\n",
            "        [0.2001, 0.3453]])\n",
            "tensor([[1.5600, 1.5209],\n",
            "        [1.2001, 1.3453]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# subtraction\n",
        "z = x - y\n",
        "z = torch.sub(x, y)\n",
        "\n",
        "# multiplication\n",
        "z = x * y\n",
        "z = torch.mul(x,y)\n",
        "\n",
        "# division\n",
        "z = x / y\n",
        "z = torch.div(x,y)"
      ],
      "metadata": {
        "id": "qXEAxoExC2aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing\n",
        "x = torch.rand(5,3)\n",
        "print(x)\n",
        "print(\"x[:, 0]\", x[:, 0]) # all rows, column 0\n",
        "print(\"x[1, :]\", x[1, :]) # row 1, all columns\n",
        "print(\"x[1, 1]\", x[1,1]) # element at 1, 1\n",
        "\n"
      ],
      "metadata": {
        "id": "KPmWwGnIClh_",
        "outputId": "36ca1861-40d2-480f-a452-df6f9352dbad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0181, 0.1100, 0.5256],\n",
            "        [0.2587, 0.3269, 0.4605],\n",
            "        [0.7458, 0.0070, 0.4519],\n",
            "        [0.3945, 0.8181, 0.2563],\n",
            "        [0.0352, 0.1937, 0.8751]])\n",
            "x[:, 0] tensor([0.0181, 0.2587, 0.7458, 0.3945, 0.0352])\n",
            "x[1, :] tensor([0.2587, 0.3269, 0.4605])\n",
            "x[1, 1] tensor(0.3269)\n",
            "x[1,1].item() 0.32685965299606323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape with torch.view()\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "\n",
        "# Print sizes\n",
        "print(x.size(), y.size(), z.size())\n"
      ],
      "metadata": {
        "id": "f2D2dznXCouO",
        "outputId": "a1845cf8-4236-4344-c081-5402df6e7363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Autograd:\n",
        "\n",
        "In PyTorch, autograd (automatic differentiation) helps us figure out how changing the inputs of our mathematical operations affects the final result.\n",
        "requires_grad=True:\n",
        "\n",
        "Autograd in PyTorch is like an automatic assistant that keeps track of how your model's performance changes with each tweak (parameter update), helping you efficiently improve your model over time.\n",
        "\n",
        "When you create a PyTorch tensor and set requires_grad=True, it's like telling PyTorch, \"Hey, keep an eye on this tensor. I might want to know how much it contributes to the final result later.\"\n",
        "This is crucial in machine learning. When you're training a model, you want to know how much each input (like a parameter or feature) affects the output (like the prediction). Setting requires_grad=True helps PyTorch automatically track these relationships.\n",
        "\n",
        "Set `requires_grad = True`:"
      ],
      "metadata": {
        "id": "AUNVPAlvxqlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model, Loss & Optimizer\n",
        "\n",
        "\n",
        "   1.**Forward Pass:**\n",
        "•\tImagine you're teaching a model. During the forward pass, the model makes predictions based on the input data. These predictions might be good or bad.\n",
        "2.\t**Loss Calculation:**\n",
        "•\tYou compare the model's predictions to the actual (true) values. This difference is the \"error\" or \"loss.\" The goal is to minimize this loss.\n",
        "3.\t**Backward Pass (Backpropagation):**\n",
        "•\tBackpropagation is like giving feedback to the model. It asks, \"For each parameter in the model, how much did it contribute to the error?\"\n",
        "•\t**Gradients are calculated.** Gradients tell us the direction and magnitude of change needed in each parameter to reduce the error.\n",
        "4.**Update Parameters:**\n",
        "•\tWith the gradients in hand, you update the model's parameters to reduce the error. This step is where learning happens.\n",
        "•\tThe model is now a bit better at making predictions.\n",
        "\n",
        "\n",
        "A typical PyTorch pipeline looks like this:\n",
        "\n",
        "1. Design model (input, output, forward pass with different layers)\n",
        "2. Construct loss and optimizer\n",
        "3. Training loop:\n",
        "  - Forward = compute prediction and loss\n",
        "  - Backward = compute gradients\n",
        "  - Update weights"
      ],
      "metadata": {
        "id": "Vjq9HmRFQjov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x\n",
        "# here : f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f'n_samples = {n_samples}, n_features = {n_features}')\n",
        "\n",
        "# 0) create a test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "e9HD5KW3QGFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LiI7rPmZczDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design Model, the model has to implement the forward pass!\n",
        "\n",
        "# Here we could simply use a built-in model from PyTorch\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# model class (LinearRegression) always inherit from nn module.\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "\n",
        "        # define different layers apply in our model\n",
        "        # in linear regression we use just one layer so we write nn linear regression.\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      # in the forward pass we pass the linear regression parameter\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "input_size, output_size = n_features, n_features\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f({X_test.item()}) = {model(X_test).item():.3f}')\n",
        "\n",
        "# 2) Define loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) Training loop\n",
        "\n",
        "#, an epoch is one complete pass through the entire training dataset during the training of a model.\n",
        "for epoch in range(n_epochs):\n",
        "    # predict = forward pass with our model\n",
        "    y_predicted = model(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients after updating\n",
        "\n",
        "    # this optimizer step will update the model paramter\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        w, b = model.parameters() # unpack parameters\n",
        "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l.item())\n",
        "\n",
        "print(f'Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "id": "eID-YTrgqNUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. First Neural Net\n",
        "\n",
        "implementation of a simple neural network for binary classification using PyTorch.\n",
        "\n",
        "Neural Network Architecture:\n",
        "\n",
        "•\tThe code defines a neural network class (SimpleNN) with one input layer, one hidden layer with ReLU activation, and one output layer.\n",
        "•\tThe neural network is designed for binary classification with two input features.\n",
        "•\tDataset Creation:\n",
        "\n",
        "•\tA small dataset (X) with input features and corresponding labels (Y) is created.\n",
        "•\tModel Initialization:\n",
        "\n",
        "•\tAn instance of the SimpleNN model is created with specified input, hidden, and output sizes.\n",
        "•\tThe model will learn to map input features to binary labels.\n",
        "•\tLoss Function and Optimizer:\n",
        "\n",
        "•\tThe Binary Cross Entropy with Logits loss (nn.BCEWithLogitsLoss()) is used, suitable for binary classification problems.\n",
        "•\tThe Adam optimizer (optim.Adam) is employed to optimize the model's parameters during training.\n",
        "•\tTraining Loop:\n",
        "\n",
        "•\tThe model is trained for a specified number of epochs.\n",
        "•\tIn each epoch, predictions are made using the current model, and the loss is calculated by comparing predictions to actual labels.\n",
        "•\tThe optimizer is then used to perform a backward pass to compute gradients and update the model parameters.\n",
        "•\tTesting the Trained Model:\n",
        "\n",
        "•\tAfter training, the model is tested on a separate set of data (test_data).\n",
        "•\tPredictions are made, and the results are printed, showing the input features and corresponding model predictions.\n",
        "•\tActivation Function:\n",
        "\n",
        "•\tThe ReLU activation function is used in the hidden layer to introduce non-linearity.\n",
        "•\tPrinted Outputs:\n",
        "\n",
        "\n",
        "\n",
        "The code prints the loss during training at intervals of 100 epochs.\n",
        "It also prints the predictions on the test data after training.\n",
        "In summary, the code demonstrates the complete lifecycle of a simple neural network: from defining the architecture to training and testing for binary classification. The model is trained to make predictions on the XOR dataset (a classic binary classification problem).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M4njny3PUJsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # Define the first fully connected layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # Apply the Rectified Linear Unit (ReLU) activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # Define the second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Define the forward pass of the network\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create a dataset with input features (X) and corresponding labels (Y)\n",
        "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=torch.float32)\n",
        "Y = torch.tensor([[0.0], [1.0], [1.0], [0.0]], dtype=torch.float32)\n",
        "\n",
        "# Initialize the neural network with specified input, hidden, and output sizes\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define the loss function (Binary Cross Entropy with Logits) and the optimizer (Adam)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 1000\n",
        "for epoch in range(n_epochs):\n",
        "    # Forward pass: make predictions using the neural network\n",
        "    predictions = model(X)\n",
        "    # Compute the loss by comparing predictions to actual labels\n",
        "    loss = criterion(predictions, Y)\n",
        "\n",
        "    # Backward pass: compute gradients and update model parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the trained model\n",
        "with torch.no_grad():\n",
        "    # Create a test dataset\n",
        "    test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=torch.float32)\n",
        "    # Make predictions on the test data and apply sigmoid for binary classification\n",
        "    predictions = model(test_data)\n",
        "\n",
        "    print(\"\\nPredictions:\")\n",
        "    for i in range(len(test_data)):\n",
        "        # Print input features and corresponding predictions\n",
        "        print(f\"Input: {test_data[i].numpy()}, Prediction: {torch.sigmoid(predictions[i]).item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Wk4gdclP9yPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f37e13-59e2-43e6-e81f-85ac43f657d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Loss: 0.4384\n",
            "Epoch 200/1000, Loss: 0.1615\n",
            "Epoch 300/1000, Loss: 0.0614\n",
            "Epoch 400/1000, Loss: 0.0323\n",
            "Epoch 500/1000, Loss: 0.0202\n",
            "Epoch 600/1000, Loss: 0.0140\n",
            "Epoch 700/1000, Loss: 0.0103\n",
            "Epoch 800/1000, Loss: 0.0079\n",
            "Epoch 900/1000, Loss: 0.0063\n",
            "Epoch 1000/1000, Loss: 0.0052\n",
            "\n",
            "Predictions:\n",
            "Input: [0. 0.], Prediction: 0.0025\n",
            "Input: [0. 1.], Prediction: 0.9855\n",
            "Input: [1. 0.], Prediction: 0.9985\n",
            "Input: [1. 1.], Prediction: 0.0020\n"
          ]
        }
      ]
    }
  ]
}