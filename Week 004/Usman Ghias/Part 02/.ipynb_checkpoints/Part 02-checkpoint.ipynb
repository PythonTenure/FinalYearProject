{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f193e8-2eff-4665-8518-bc83ba079fdf",
   "metadata": {},
   "source": [
    "# $$ Part 01 - Refresh $$\n",
    "### To check comments visit Part 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417522f-8b45-4ed4-afac-0f8987bea619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install patchify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d01de3-3cab-4b1b-9162-6439ca0b1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image \n",
    "import numpy as np \n",
    "from patchify import patchify\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e8b9b-913a-4f45-abc0-f55d7be6807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxscaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371aa55-16fe-4a9e-ae2c-f66243b0fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_folder = '/content/drive/MyDrive/Colab Notebooks/datasets/satellite/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c226b-aed4-4ecc-9e5f-2f8cfb2b5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"DubaiDataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ada1e1-f9c1-4175-9a40-004aaacb4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbc464-fe5e-4cf3-9d92-af85a23c44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "for image_type in ['images' , 'masks']:\n",
    "  if image_type == 'images':\n",
    "    image_extension = 'jpg'\n",
    "  elif image_type == 'masks':\n",
    "     image_extension = 'png'\n",
    "  for tile_id in range(1,8):\n",
    "    for image_id in range(1,20):\n",
    "      image = cv2.imread(f'{dataset_root_folder}/{dataset_name}/Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}',1)\n",
    "      if image is not None:\n",
    "        if image_type == 'masks':\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #print(image.shape)\n",
    "        size_x = (image.shape[1]//image_patch_size)*image_patch_size\n",
    "        size_y = (image.shape[0]//image_patch_size)*image_patch_size\n",
    "        #print(\"{} --- {} - {}\".format(image.shape, size_x, size_y))\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.crop((0,0, size_x, size_y))\n",
    "        #print(\"({},  {})\".format(image.size[0],image.size[1]))\n",
    "        image = np.array(image)\n",
    "        patched_images = patchify(image, (image_patch_size, image_patch_size, 3), step=image_patch_size)\n",
    "        #print(len(patched_images))\n",
    "        for i in range(patched_images.shape[0]):\n",
    "          for j in range(patched_images.shape[1]):\n",
    "            if image_type == 'images':\n",
    "              individual_patched_image = patched_images[i,j,:,:]\n",
    "              #print(individual_patched_image.shape)\n",
    "              individual_patched_image = minmaxscaler.fit_transform(individual_patched_image.reshape(-1, individual_patched_image.shape[-1])).reshape(individual_patched_image.shape)\n",
    "              individual_patched_image = individual_patched_image[0]\n",
    "              #print(individual_patched_image.shape)\n",
    "              image_dataset.append(individual_patched_image)\n",
    "            elif image_type == 'masks':\n",
    "              individual_patched_mask = patched_images[i,j,:,:]\n",
    "              individual_patched_mask = individual_patched_mask[0]\n",
    "              mask_dataset.append(individual_patched_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1bc71-d906-4712-8f1b-4b0bce331e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset = np.array(mask_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967b3aa-3224-4c5d-8df1-2ee7672a206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_dataset))\n",
    "print(len(mask_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bbbff-20db-4109-9b56-49d8024e39ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_id = random.randint(0, len(image_dataset))\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_dataset[random_image_id])\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_dataset[random_image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b9f36-b605-4de6-9fdc-e31716de52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_building = '#3C1098'\n",
    "class_building = class_building.lstrip('#')\n",
    "class_building = np.array(tuple(int(class_building[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_building)\n",
    "\n",
    "class_land = '#8429F6'\n",
    "class_land = class_land.lstrip('#')\n",
    "class_land = np.array(tuple(int(class_land[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_land)\n",
    "\n",
    "class_road = '#6EC1E4'\n",
    "class_road = class_road.lstrip('#')\n",
    "class_road = np.array(tuple(int(class_road[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_road)\n",
    "\n",
    "class_vegetation = '#FEDD3A'\n",
    "class_vegetation = class_vegetation.lstrip('#')\n",
    "class_vegetation = np.array(tuple(int(class_vegetation[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_vegetation)\n",
    "\n",
    "class_water = '#E2A929'\n",
    "class_water = class_water.lstrip('#')\n",
    "class_water = np.array(tuple(int(class_water[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_water)\n",
    "\n",
    "class_unlabeled = '#9B9B9B'\n",
    "class_unlabeled = class_unlabeled.lstrip('#')\n",
    "class_unlabeled = np.array(tuple(int(class_unlabeled[i:i+2], 16) for i in (0,2,4)))\n",
    "print(class_unlabeled)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3983e4d-9692-46dd-a201-38366df7a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = individual_patched_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e178dc3-5131-4489-bc89-c13860e9753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_label(label):\n",
    "  label_segment = np.zeros(label.shape, dtype=np.uint8)\n",
    "  label_segment[np.all(label == class_water, axis=-1)] = 0\n",
    "  label_segment[np.all(label == class_land, axis=-1)] = 1\n",
    "  label_segment[np.all(label == class_road, axis=-1)] = 2\n",
    "  label_segment[np.all(label == class_building, axis=-1)] = 3\n",
    "  label_segment[np.all(label == class_vegetation, axis=-1)] = 4\n",
    "  label_segment[np.all(label == class_unlabeled, axis=-1)] = 5\n",
    "  #print(label_segment)\n",
    "  label_segment = label_segment[:,:,0]\n",
    "  #print(label_segment)\n",
    "  return label_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb19d7-c3cd-47a9-b4f4-4f44f91494cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "for i in range(mask_dataset.shape[0]):\n",
    "  label = rgb_to_label(mask_dataset[i])\n",
    "  labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf7bf9-9939-4006-b96e-7d2990207226",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "labels = np.expand_dims(labels, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ea4f1-6a7f-40b7-a570-6175b663816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.unique(labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b6257-44af-4d54-8873-f8431b6baa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total unique labels based on masks: \",format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462da55f-cab8-431b-b3a1-a604683da150",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_id = random.randint(0, len(image_dataset))\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image_dataset[random_image_id])\n",
    "plt.subplot(122)\n",
    "#plt.imshow(mask_dataset[random_image_id])\n",
    "plt.imshow(labels[random_image_id][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3467e-fddf-4ecd-960c-d08c73e8cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes = len(np.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6364e8-281b-44a3-b47c-79874e065e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af21c9-8488-454e-89a9-06d5eacc61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ce624-4a39-43f7-a5ee-0f4efbcec307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_categorical_dataset = to_categorical(labels, num_classes=total_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb181954-d946-4066-a792-4db8aaf9fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_trianing_dataset = image_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba62181-21aa-4da1-baa4-59908bf478cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3995c-a9f9-452f-8275-2b98045cbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(master_trianing_dataset, labels_categorical_dataset, test_size=0.15, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596f0e0-58f2-4c90-a959-ee82ca492511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11549046-0720-4ef1-94ac-a981c61bdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "image_channels = X_train.shape[3]\n",
    "total_classes = y_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d6acc-e4ed-429f-bb8b-21013e9021d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(image_height)\n",
    "print(image_width)\n",
    "print(image_channels)\n",
    "print(total_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140b157-579a-49fa-8315-9f9dc09e5e03",
   "metadata": {},
   "source": [
    "# $$ Part 01 -Starts Here $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b155b-be96-470c-a54c-06dfcddc6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install or upgrade the segmentation-models library, which is used for semantic segmentation tasks.\n",
    "!pip install -U segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e70d41-2339-4362-9377-8ad0e50ac1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential modules for building a convolutional neural network (CNN) model.\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import concatenate, BatchNormalization, Dropout, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f152e1-dfa0-470e-a13b-59a87e46e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras backend module as 'K'.\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58292870-38cc-4052-86d6-b48f43a8ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "  # Flatten the true and predicted segmentation masks to 1D arrays.\n",
    "  y_true_flatten = K.flatten(y_true)\n",
    "  y_pred_flatten = K.flatten(y_pred)\n",
    "\n",
    "  # Calculate the intersection between true and predicted mask values.\n",
    "  intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "\n",
    "  # Compute the Jaccard coefficient, also known as Intersection over Union (IoU).\n",
    "  final_coef_value = (intersection + 1.0) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n",
    "\n",
    "  return final_coef_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbb2aa-4d0d-46b9-ad0b-2835b04fc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a U-Net model for image segmentation.\n",
    "def multi_unet_model(n_classes=5, image_height=256, image_width=256, image_channels=1):\n",
    "    # Define the input layer for the model.\n",
    "    inputs = Input((image_height, image_width, image_channels))\n",
    "\n",
    "    # Create a reference to the input layer for future concatenation.\n",
    "    source_input = inputs\n",
    "\n",
    "    # Contracting path (Encoder)\n",
    "    c1 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(source_input)\n",
    "    c1 = Dropout(0.2)(c1)\n",
    "    c1 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c1)\n",
    "    p1 = MaxPooling2D((2,2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p1)\n",
    "    c2 = Dropout(0.2)(c2)\n",
    "    c2 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c2)\n",
    "    p2 = MaxPooling2D((2,2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c3)\n",
    "    p3 = MaxPooling2D((2,2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c4)\n",
    "    p4 = MaxPooling2D((2,2))(c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n",
    "    c5 = Dropout(0.2)(c5)\n",
    "    c5 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n",
    "\n",
    "    # Expansive path (Decoder)\n",
    "    u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding=\"same\")(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding=\"same\")(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding=\"same\")(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u8)\n",
    "    c8 = Dropout(0.2)(c8)\n",
    "    c8 = Conv2D(32, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding=\"same\")(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u9)\n",
    "    c9 = Dropout(0.2)(c9)\n",
    "    c9 = Conv2D(16, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c9)\n",
    "\n",
    "    # Output layer for segmentation masks.\n",
    "    outputs = Conv2D(n_classes, (1,1), activation=\"softmax\")(c9)\n",
    "\n",
    "    # Create and return the U-Net model.\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367d516-bf5f-46c1-a189-d78965a6ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of evaluation metrics to be used during model training and validation.\n",
    "# In this case, two metrics are defined: \"accuracy\" and \"jaccard_coef\".\n",
    "\n",
    "# \"accuracy\" measures the overall accuracy of the model's predictions.\n",
    "# It's a common metric for classification tasks where the goal is to correctly classify each pixel.\n",
    "\n",
    "# \"jaccard_coef\" computes the Jaccard coefficient (intersection over union) for segmentation masks.\n",
    "# This metric measures the similarity between the predicted and true segmentation masks.\n",
    "metrics = [\"accuracy\", jaccard_coef]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6722c-dd83-4b86-89ee-1467296e2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the height of the input images.\n",
    "print(image_height)\n",
    "\n",
    "# Print the width of the input images.\n",
    "print(image_width)\n",
    "\n",
    "# Print the number of color channels in the input images (e.g., 1 for grayscale, 3 for RGB).\n",
    "print(image_channels)\n",
    "\n",
    "# Print the total number of classes for image segmentation.\n",
    "print(total_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57b331-be71-45fa-93ff-dbc995f70ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a deep learning model for image segmentation.\n",
    "def get_deep_learning_model():\n",
    "  # Calls the 'multi_unet_model' function to create a U-Net model for image segmentation.\n",
    "  return multi_unet_model(n_classes=total_classes, \n",
    "                          image_height=image_height, \n",
    "                          image_width=image_width, \n",
    "                          image_channels=image_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7fb0d-b938-4691-98d3-0d94f60f11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the deep learning model using the defined function 'get_deep_learning_model'.\n",
    "model = get_deep_learning_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01b467-47e7-4ac6-ae60-98700e1577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomment this line to get model confiuration\n",
    "# model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b88826-7b41-4aa2-8876-73c9d2073cac",
   "metadata": {},
   "source": [
    "# Generating Loss Function\n",
    "- dice loss > Focal Loss > Total Loss\n",
    "- Total Loss = (Dice loss + (1*Focal Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933d628-5eb6-467b-adc2-6fb2a7174399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of weights, each representing the relative weight for a specific class.\n",
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff27246-7fb2-490a-866d-7f984cc6755d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bec35c-5ece-4af3-a276-e6c2f3c71e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Dice Loss object\n",
    "# This is a loss function commonly used in image segmentation tasks.\n",
    "dice_loss = sm.losses.DiceLoss(\n",
    "    class_weights=weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369d9e4-a49a-4f27-9484-2490363dd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Focal Loss object for multi-class image segmentation.\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec0df5-dddb-4236-a670-87e8bded50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Segmentation Models library (segmentation_models) as \"sm.\"\n",
    "import segmentation_models as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1859a-4b29-413c-b786-4d4819a1d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total loss as a combination of Dice Loss and Focal Loss.\n",
    "total_loss = dice_loss + (1 * focal_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd66ce7-fdd7-4681-87d2-f1ae70c30ce6",
   "metadata": {},
   "source": [
    "# Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd8f5a-b179-4b4f-affa-46d0a25303cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58074b-9a02-47b9-b501-efd78fe3b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the TensorFlow Keras backend session to release any allocated memory and resources.\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7e138-23d7-47c1-8e1f-4cfd47943922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the deep learning model using the Adam optimizer, total loss function, and specified evaluation metrics.\n",
    "model.compile(optimizer=\"adam\", loss=total_loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962320d-f39c-4714-a0c8-95773f061ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the deep learning model's architecture.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162fae0-e64c-4060-8b40-a4d9000227a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the deep learning model on the training data.\n",
    "model_history = model.fit(X_train, y_train,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          epochs=100,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad4368-9fa1-4f1f-bed0-a10dda8e3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training and validation loss values from the training history.\n",
    "loss = history_a.history['loss']\n",
    "val_loss = history_a.history['val_loss']\n",
    "\n",
    "# Create a plot of training and validation loss over epochs.\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label=\"Training Loss\")  # Plot training loss in yellow\n",
    "plt.plot(epochs, val_loss, 'r', label=\"Validation Loss\")  # Plot validation loss in red\n",
    "\n",
    "# Add labels and a legend to the plot.\n",
    "plt.title(\"Training Vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562cab2-383c-42e8-a7e2-c54217b74115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training and validation IoU (Jaccard coefficient) values from the training history.\n",
    "jaccard_coef = history_a.history['jaccard_coef']\n",
    "val_jaccard_coef = history_a.history['val_jaccard_coef']\n",
    "\n",
    "# Create a plot of training and validation IoU over epochs.\n",
    "epochs = range(1, len(jaccard_coef) + 1)\n",
    "plt.plot(epochs, jaccard_coef, 'y', label=\"Training IoU\")  # Plot training IoU in yellow\n",
    "plt.plot(epochs, val_jaccard_coef, 'r', label=\"Validation IoU\")  # Plot validation IoU in red\n",
    "\n",
    "# Add labels and a legend to the plot.\n",
    "plt.title(\"Training Vs Validation IoU\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"IoU (Jaccard Coefficient)\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3b41b-ae45-4d83-952f-536694648b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the model's training parameters\n",
    "model_history.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7117-09fd-416c-8f69-8d766b68a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data using the trained model\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387b5b0-d266-4a2a-8451-66931c718942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of predictions in the y_pred array\n",
    "len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8246b94-4fe3-40e6-8026-7b6bfa6d6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred contains the predictions made by the deep learning model\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e03f8f-7013-4301-8b52-743b424c6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_argmax contains the class labels with the highest probability for each pixel\n",
    "y_pred_argmax = np.argmax(y_pred, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02336f91-e42d-4c81-9eb9-ee98ce35b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length (number of elements) in the y_pred_argmax array\n",
    "len(y_pred_argmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8040dc9-e72e-4627-9273-0216a9da1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the y_pred_argmax array\n",
    "y_pred_argmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ca77e-1963-4945-96f3-64d878cb8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the argmax of y_test to get the true class labels\n",
    "y_test_argmax = np.argmax(y_test, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4513e2e-0028-4654-aacb-cb050a96caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the true class labels from y_test_argmax\n",
    "y_test_argmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94194afe-9d30-46bc-b41e-8d124c6929d7",
   "metadata": {},
   "source": [
    "# Comparing prediction results\n",
    "- Using test images using mask images and predicted result images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b48c0-a676-42d2-84f1-4a84aa45daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9b57d-fb96-40a6-978f-f594750b99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random test image from the test dataset.\n",
    "test_image_number = random.randint(0, len(X_test))\n",
    "test_image = X_test[test_image_number]\n",
    "\n",
    "# Retrieve the corresponding ground truth (actual) segmentation mask.\n",
    "ground_truth_image = y_test_argmax[test_image_number]\n",
    "\n",
    "# Prepare the test image input for prediction by expanding its dimensions.\n",
    "test_image_input = np.expand_dims(test_image, 0)\n",
    "\n",
    "# Use the model to predict the segmentation mask for the test image.\n",
    "prediction = model.predict(test_image_input)\n",
    "\n",
    "# Extract the predicted segmentation mask and convert it to a 2D format.\n",
    "predicted_image = np.argmax(prediction, axis=3)\n",
    "predicted_image = predicted_image[0, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35468a-5a81-4d64-852a-3ea160a8d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of the original image, ground truth mask, and predicted mask.\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Display the original image.\n",
    "plt.subplot(231)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(test_image)\n",
    "\n",
    "# Display the original masked image (ground truth).\n",
    "plt.subplot(232)\n",
    "plt.title(\"Original Masked image\")\n",
    "plt.imshow(ground_truth_image)\n",
    "\n",
    "# Display the predicted segmentation mask.\n",
    "plt.subplot(233)\n",
    "plt.title(\"Predicted Image\")\n",
    "plt.imshow(predicted_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4191c-c71c-41cb-84e5-4767dd1e85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"satellite_segmentation_full.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54558aca-ec4a-4b68-b562-51f34550c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List files and directories in the current directory with human-readable sizes.\n",
    "!ls -lah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f56da-f564-4585-b8db-197c433b7495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa9b6e-66c1-4064-a03a-35204b6dc159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233825f6-eea9-4252-8058-3b3dc2009317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ab7bd-493d-472d-8f22-a37fa82f5132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8bb40-b8b8-4b8a-9092-a8a4b5856192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c619474-ca54-4ca0-ad43-b589ab988f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735c14c-b9f2-4172-8d2c-a902acfb4d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
